{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DDI InfluxDB Timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os \n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import time\n",
    "from influxdb_client import Point\n",
    "from libs.fetch_data import fetch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Load of env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# URLs und Token laden wir weiter aus der .env (das funktioniert ja)\n",
    "URL   = os.getenv(\"INFLUX_URL\")\n",
    "ORG   = os.getenv(\"INFLUX_ORG\", \"basel-lab\")\n",
    "TOKEN = os.getenv(\"INFLUX_TOKEN\")\n",
    "PARKING_URL = os.getenv(\"PARKING_URL\")\n",
    "RAIN_URL = os.getenv(\"RAIN_URL\")\n",
    "ALRDY_INSTALLED = os.getenv(\"ALRDY_INSTALLED\", 'False').lower() in ('true', '1', 't') \n",
    "\n",
    "# WICHTIG: Hier keine getenv() Funktion nutzen, sondern den Namen direkt zuweisen!\n",
    "BUCKET = \"parking_data_final\"\n",
    "\n",
    "print(f\"Der Bucket ist gesetzt auf: '{BUCKET}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Fetch of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALRDY_INSTALLED:\n",
    "    df_parking, df_rain = fetch_data(PARKING_URL, RAIN_URL)\n",
    "    df_parking.to_csv(\"../data/parking.csv\")\n",
    "    df_rain.to_csv(\"../data/rain.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Data analysis for import in InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_parking = pd.read_csv(\"../data/parking.csv\", sep=\",\")\n",
    "df_rain = pd.read_csv(\"../data/rain.csv\", sep=\",\")\n",
    "\n",
    "ts = pd.to_datetime(df_parking['published'], errors=\"coerce\", utc=True)\n",
    "df_parking['time'] = ts\n",
    "\n",
    "\n",
    "df_rain['date'] = pd.to_datetime(df_rain['date'], errors=\"coerce\")\n",
    "df_rain['time'] = (\n",
    "    df_rain['date']\n",
    "    + pd.to_timedelta(12, unit=\"h\")\n",
    ").dt.tz_localize(\"Europe/Zurich\", nonexistent=\"shift_forward\", ambiguous=\"NaT\"\n",
    ").dt.tz_convert(\"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rain.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parking.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Make Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection \n",
    "assert TOKEN, \"Bitte Token in .env file hinzuf端gen\"\n",
    "client = InfluxDBClient(url=URL, token=TOKEN, org=ORG)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "query_api = client.query_api()\n",
    "health = client.health()\n",
    "print(\"InfluxDB health:\", health.status, \"-\", health.message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Daten in das Model schreiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# KONFIGURATION\n",
    "# ==========================================\n",
    "# Wir reduzieren die Menge pro Paket drastisch, um Timeouts zu verhindern\n",
    "BATCH_SIZE = 1000  \n",
    "PAUSE_SEC = 0.1    # Kurze Pause zwischen den Batches\n",
    "\n",
    "# ==========================================\n",
    "# 1. PARKDATEN\n",
    "# ==========================================\n",
    "print(\"Erstelle 'Points' f端r Parkdaten...\")\n",
    "\n",
    "points_parking = []\n",
    "for index, row in df_parking.iterrows():\n",
    "    if pd.notnull(row['time']): \n",
    "        p = Point(\"parking\") \\\n",
    "            .tag(\"parking_id\", str(row['id'])) \\\n",
    "            .tag(\"parking_name\", str(row['title'])) \\\n",
    "            .field(\"free\", float(row['free'])) \\\n",
    "            .field(\"total\", float(row['total'])) \\\n",
    "            .time(row['time'])\n",
    "        points_parking.append(p)\n",
    "\n",
    "print(f\"-> {len(points_parking)} Park-Punkte bereit. Starte Upload in {BATCH_SIZE}er Schritten...\")\n",
    "\n",
    "# Sicherer Batch-Loop\n",
    "for i in range(0, len(points_parking), BATCH_SIZE):\n",
    "    batch = points_parking[i : i + BATCH_SIZE]\n",
    "    write_api.write(bucket=BUCKET, org=ORG, record=batch)\n",
    "    print(f\"   Park-Batch {i} bis {i + len(batch)} geschrieben.\")\n",
    "    time.sleep(PAUSE_SEC) # Kurze Atempause f端r die DB\n",
    "\n",
    "print(\"Parkdaten fertig hochgeladen.\\n\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. WETTERDATEN\n",
    "# ==========================================\n",
    "print(\"Erstelle 'Points' f端r Wetterdaten...\")\n",
    "\n",
    "points_rain = []\n",
    "for index, row in df_rain.iterrows():\n",
    "    if pd.notnull(row['time']):\n",
    "        regen = float(row['rre150d0']) if pd.notnull(row['rre150d0']) else 0.0\n",
    "        temp = float(row['tre200d0']) if pd.notnull(row['tre200d0']) else 0.0\n",
    "        \n",
    "        p = Point(\"weather\") \\\n",
    "            .tag(\"location\", \"Basel\") \\\n",
    "            .field(\"precipitation\", regen) \\\n",
    "            .field(\"temperature\", temp) \\\n",
    "            .time(row['time'])\n",
    "        points_rain.append(p)\n",
    "\n",
    "print(f\"-> {len(points_rain)} Wetter-Punkte bereit. Starte Upload in {BATCH_SIZE}er Schritten...\")\n",
    "\n",
    "# Sicherer Batch-Loop\n",
    "for i in range(0, len(points_rain), BATCH_SIZE):\n",
    "    batch = points_rain[i : i + BATCH_SIZE]\n",
    "    write_api.write(bucket=BUCKET, org=ORG, record=batch)\n",
    "    print(f\"   Wetter-Batch {i} bis {i + len(batch)} geschrieben.\")\n",
    "    time.sleep(PAUSE_SEC)\n",
    "\n",
    "print(\"\\n------------------------------------------------\")\n",
    "print(\"FERTIG! Alle Daten wurden erfolgreich importiert.\")\n",
    "print(\"------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddi",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
